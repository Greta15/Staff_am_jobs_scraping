# -*- coding: utf-8 -*-
"""Staff.am Jobs scraping(3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bQcqhxM6Zmgxzj5SA397mrYxN4SdI7Ks
"""

import time
import requests
import numpy as np
import pandas as pd
import re
!pip install scrapy
from scrapy.http import TextResponse

"""# **Staff.am Jobs scraping**"""

URL="https://staff.am/en/jobs"

page=requests.get(URL)

type(page)

response= TextResponse(body=page.text,url=URL,encoding="utf-8")

def request(url):
    page = requests.get(url)
    response = TextResponse(body=page.text,url=url,encoding="utf-8")
    return response

def jobsto_scrape(url):
    page = requests.get(url)
    response= TextResponse(body=page.text,url=url,encoding="utf-8")
    
    vacancy_name=response.css("div.job-inner.job-item-title").extract()
    vacancy=[i.css('p.font_bold::text').extract_first() for i in response.css("div.job-inner.job-item-title")]
    company_name=response.css("div.job-inner.job-item-title").extract()
    company=[i.css('p.font_bold ~ p.job_list_company_title::text').extract_first() for i in response.css("div.job-inner.job-item-title")]
    base_url2 = "https://staff.am"
    job_page_URL = response.css("div.web_item_card.hs_job_list_item>a::attr(href)").extract()
    job_page_URL2 = [base_url2 + i for i in job_page_URL ]
    job_location = response.css("div[class = 'job-inner job-list-deadline'] >p[class='job_location']")
    job_loc = [i.css("::text").extract()[1] for i in job_location]
    location = [i.replace('\n',"").strip() for i in job_loc]
    deadline = response.css("div[class = 'job-inner job-list-deadline'] >p:not([class='job_location'])")
    post_deadline2 =[i.css('::text').extract()[1] for i in deadline]
    posting_deadline =[i.replace('\n'," ") for i in post_deadline2]
    return pd.DataFrame({"vacancy":vacancy,"company":company,'job_page_URL2':job_page_URL2,"posting_deadline":posting_deadline,'location':location})
staff_jobs = []
for i in range(1,20):
    current_page =jobsto_scrape(url = f"https://staff.am/en/jobs?page={i}&per-page=50")
    if current_page.shape[0] == 0:
        break
    else:
        staff_jobs.append(current_page)
staff_jobs = pd.concat(staff_jobs)
staff_jobs

"""*   **The number of times the specified company appears( current job posting count)**"""

staff_jobs['company']

staff_jobs['company'].count

staff_jobs['company'].value_counts()

"""*   **The number of jobs posted by that companies**

1.   Digitain- 32
2.   SoftConstruct-23

1.   Picsart-21
2.   ServiceTitan-16

*   **Jobs including word 'data' inside**
"""

[staff_jobs['vacancy']]

staff_jobs1=staff_jobs[staff_jobs['vacancy'].str.contains("Data")]
print (staff_jobs1)

(staff_jobs1['vacancy']).count()

"""*9 jobs have the word "Data" inside* ( it was 9 when i checked the last time)
Data Scientist  
Data Engineer (Spark)  
Data analyst  
Data Engineer  
Junior Database Administrator  
Senior Database Administrator  
Data Specialist / QA Engineer  
Database Administrator  
Engineering Manager, Java Microservices & Data
"""